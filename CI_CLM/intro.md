## Continuous Integration (CI) for Machine Learning using CML library 
### Background on MLOps

MLOps has gained some traction during the past few years; it has been conceived to facilitate the development and deployment of ML models. 
One key feature of ML models is the need to continuosly adapt them according to the problem at hand. Some possible scenarios might be:
* Having to incorporate more relevant data for the training of the model, in order to obtain more accurate results and avoid using a model based on outdated data, that would probably be higly inaccurate in real world scenarios.
* Having to change the internal structure of the model, because it just so happens to perform better than the previous version.
* Having to build a model that uses the same input data but that has to predict something different (hence we need to make some changes to the part of the code that takes into account the output.

![image](https://user-images.githubusercontent.com/63954877/168755153-8fa197ce-faf9-40a0-9182-42dddcff51b8.png)

All of the cases mentioned represent possible delays from the development to the deployment of the ML model in production, and practitioners often have to spend considerable amounts of time to fix these issues, because until 2015 MLOps was not invented.
MLOps offers a set of tools and practices that aim at including ML inside the DevOps practices, and hence tries to facilitate and automate the delivery of the aforemntioned models in a transparent and agile way.

### What is CML

CML (Continuous Machine Learning) is an open-source library created to allow the implementation of CI/CD pipelines in Machine Learning projects. It can be used to automate the development and deployment of ML models, facilitating users to compare the performance of various ML models, directly on GitHub.

![image](https://user-images.githubusercontent.com/63954877/168757979-6f02bd86-f001-4604-a568-e907d91fdb42.png)

One important issue that may arise when working on a ML project on a GitHub repository is that, when new changes to the code (for example in the structure or type of model, its hyperparameters, etc.) are pushed, there is not an intuitive and clear way to aknowledge if the changes actually improved the performance of the model. In fact what would normally happen is that each and every person contributing to the same project should have to pull the code on their machine, run the training and see with their own eyes if the accuracy improved.

Some key points that is crucial to have when training a ML model are:
* Model accuracy on training and validation set
* Loss function on training and validation set
* Some output samples of the model after we feed some new data
* All of the points above for different types of models.

What CLI can offer is a simple way to avoid all of this, and make sure that everyone in the team can see the results generated by the changes in the code, directly on the GitHub page of the project. This can greatly improve the efficiency fo the workflow and hence facilitate obtaining better results in much less time.

In this simple tutorial we will show you how, with just a few lines of code, we can print out a markdown file containing all the relevant information (model accuracy, loss score, graphs, etc.) directly on the PR page. So that everyone on the team has a fast and direct way to analyze whether there have been improvements with the new versions of the ML model.

## Intended learning outcomes 
* Introduction to CML library 
*	How to create a CI pipeline for ML using GitHub actions
*	How to develop your own markdown report for your ML project

